{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "import torch_geometric.datasets as datasets\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, global_mean_pool,GINConv\n",
    "import random\n",
    "from torch_geometric.data import Data, DataLoader, InMemoryDataset\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = 'dataset'\n",
    "dataset = torch_geometric.datasets.TUDataset(root=DATASET_PATH, name=\"MSRC_21\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_NODES = 10  # Minimum number of nodes a subgraph must have\n",
    "MIN_EDGES = 8  # Minimum number of edges a subgraph must have\n",
    "def create_bfs_subgraphs(G, original_features, graph_label, depth_limit=8):\n",
    "    visited = set()  # Set to store visited nodes\n",
    "    subgraphs = []  # List to store subgraphs from each BFS traversal\n",
    "\n",
    "    # BFS implementation\n",
    "    def bfs(G, start_node, depth_limit):\n",
    "        bfs_nodes = set()  # To store nodes visited in this BFS\n",
    "        bfs_edges = set()  # To store edges traversed in this BFS\n",
    "        queue = [(start_node, 0)]  # (node, depth)\n",
    "        visited.add(start_node)  # Mark the start node as visited\n",
    "\n",
    "        while queue:\n",
    "            node, depth = queue.pop(0)\n",
    "            if depth < depth_limit:\n",
    "                for neighbor in G.neighbors(node):\n",
    "                    if neighbor not in visited:\n",
    "                        visited.add(neighbor)\n",
    "                        queue.append((neighbor, depth + 1))\n",
    "                        bfs_edges.add((node, neighbor))\n",
    "                        bfs_nodes.add(neighbor)\n",
    "            bfs_nodes.add(node)\n",
    "\n",
    "        return bfs_nodes, bfs_edges\n",
    "\n",
    "    # Perform BFS iteratively from random unvisited nodes until all nodes are visited\n",
    "    while len(visited) < len(G.nodes):\n",
    "        unvisited_nodes = list(set(G.nodes) - visited)\n",
    "        if not unvisited_nodes:\n",
    "            break\n",
    "\n",
    "        start_node = random.choice(unvisited_nodes)\n",
    "\n",
    "        # Run BFS from the chosen node\n",
    "        bfs_nodes, bfs_edges = bfs(G, start_node, depth_limit)\n",
    "\n",
    "        # **New Check for Minimum Nodes and Edges**\n",
    "        if len(bfs_nodes) < MIN_NODES or len(bfs_edges) < MIN_EDGES:\n",
    "            continue  # Skip this subgraph if it doesn't meet the criteria\n",
    "\n",
    "        # Map original indices to subgraph indices\n",
    "        node_indices = {node: i for i, node in enumerate(bfs_nodes)}\n",
    "\n",
    "        # Create edge index in subgraph format\n",
    "        subgraph_edges = []\n",
    "        for u, v in bfs_edges:\n",
    "            if u in bfs_nodes and v in bfs_nodes:\n",
    "                subgraph_edges.append((node_indices[u], node_indices[v]))\n",
    "\n",
    "        edge_index = torch.tensor(subgraph_edges, dtype=torch.long).t().contiguous()\n",
    "        features = original_features[list(bfs_nodes)]  # Extract node features for subgraph\n",
    "\n",
    "        # Create Data object for the subgraph, including `y` and original indices\n",
    "        data = Data(\n",
    "            x=features,\n",
    "            edge_index=edge_index,\n",
    "            y=torch.tensor([graph_label.item()], dtype=torch.long),\n",
    "            original_node_indices=torch.tensor(list(bfs_nodes), dtype=torch.long),\n",
    "        )\n",
    "        if data.edge_index.size() == 0:  # Check number of edges\n",
    "            print(\"Skipping subgraph with empty edge_index\")\n",
    "        else:\n",
    "            subgraphs.append(data)\n",
    "\n",
    "    return subgraphs\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class SubgraphDataset(InMemoryDataset):\n",
    "    def __init__(self, dataset):\n",
    "        super(SubgraphDataset, self).__init__(root=DATASET_PATH)\n",
    "        self.data_list = []\n",
    "        self.labels = []\n",
    "\n",
    "        for graph in dataset:\n",
    "            G = nx.from_edgelist(graph.edge_index.t().tolist())\n",
    "            #print(graph.y)\n",
    "            subgraphs = create_bfs_subgraphs(G, graph.x,graph_label=graph.y)  # Pass original node features\n",
    "            self.data_list.extend(subgraphs)\n",
    "            #print(subgraphs[0].y)\n",
    "            #print(graph.y)\n",
    "            self.labels.extend([graph.y] * len(subgraphs))  # Add label for each subgraph\n",
    "            \n",
    "        self.data, self.slices = self.collate(self.data_list)\n",
    "\n",
    "    def get_labels(self):\n",
    "        return torch.tensor(self.labels)\n",
    "\n",
    "\n",
    "\n",
    "train_graphs, test_graphs = train_test_split(dataset, test_size=0.2, random_state=21)\n",
    "\n",
    "\n",
    "train_subgraph_dataset = SubgraphDataset(train_graphs)\n",
    "test_subgraph_dataset = SubgraphDataset(test_graphs)\n",
    "print(len(train_subgraph_dataset),len(test_subgraph_dataset))\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_subgraph_dataset.data_list, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_subgraph_dataset.data_list, batch_size=32, shuffle=False)\n",
    "print(len(train_loader),len(test_loader))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, heads=8, dropout=0.6):\n",
    "        super(GAT, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "\n",
    "        self.conv1 = GATConv(dataset.num_node_features, hidden_channels, heads=heads, dropout=dropout)\n",
    "        \n",
    "        self.conv2 = GATConv(hidden_channels * heads, hidden_channels, heads=1, concat=False, dropout=dropout)\n",
    "        self.lin = torch.nn.Linear(hidden_channels, dataset.num_classes)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        \n",
    "        x, attn_weights = self.conv2(x, edge_index, return_attention_weights=True)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        \n",
    "        x = global_mean_pool(x, data.batch)\n",
    "\n",
    "        \n",
    "        x = self.lin(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1), attn_weights\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device : ',device)\n",
    "\n",
    "\n",
    "hidden_channels = 64\n",
    "model = GAT(hidden_channels=hidden_channels).to(device)\n",
    "print(model)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hidden_channels = 64\n",
    "heads = 8\n",
    "dropout = 0.6\n",
    "model = GAT(hidden_channels=hidden_channels, heads=heads, dropout=dropout)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "def move_to_device(batch, device):\n",
    "    batch = batch.to(device)\n",
    "    return batch\n",
    "\n",
    "# Weakly supervised training\n",
    "model.train()\n",
    "for epoch in range(20):  # Number of epochs\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        batch = move_to_device(batch, device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out, attn_weights = model(batch)\n",
    "        loss = criterion(out, batch.y)  # Compute loss using graph labels\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Accuracy calculation\n",
    "        _, predicted = torch.max(out, dim=1)\n",
    "        correct += (predicted == batch.y).sum().item()\n",
    "        total += batch.y.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = correct / total\n",
    "    print(f'Epoch {epoch + 1}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import networkx as nx\n",
    "\n",
    "# Evaluation function to select top-k subgraphs based on attention weights\n",
    "def evaluate_model_with_attention(model, dataset, k=4):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for graph in dataset:\n",
    "            # Convert the graph to a NetworkX object\n",
    "            G = nx.from_edgelist(graph.edge_index.t().tolist())\n",
    "            \n",
    "            # Generate BFS subgraphs\n",
    "            subgraphs = create_bfs_subgraphs(G, graph.x, graph_label=graph.y)\n",
    "            subgraph_outputs = []\n",
    "            subgraph_attention_scores = []\n",
    "\n",
    "            for subgraph in subgraphs:\n",
    "                subgraph = subgraph.to(device)\n",
    "                \n",
    "                # Skip subgraphs with empty edge_index\n",
    "                if subgraph.edge_index.size() == 0:  # Check number of edges\n",
    "                    print(\"Skipping subgraph with empty edge_index\")\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # Pass the subgraph to the model\n",
    "                    output, attn_weights = model(subgraph)\n",
    "                except IndexError as e:\n",
    "                    print(f\"Error processing subgraph: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                # Extract attention weights\n",
    "                if isinstance(attn_weights, (tuple, list)):\n",
    "                    attention_tensor = attn_weights[-1]\n",
    "                else:\n",
    "                    attention_tensor = attn_weights\n",
    "                \n",
    "                # Compute a single attention score for the subgraph\n",
    "                attention_score = attention_tensor.mean().item()\n",
    "                subgraph_outputs.append(output.unsqueeze(0))\n",
    "                subgraph_attention_scores.append(attention_score)\n",
    "\n",
    "            # Skip if no valid subgraph outputs are available\n",
    "            if not subgraph_outputs:\n",
    "                continue\n",
    "\n",
    "            subgraph_outputs = torch.cat(subgraph_outputs, dim=0)\n",
    "            subgraph_attention_scores = torch.tensor(subgraph_attention_scores)\n",
    "\n",
    "            # Select top-k subgraphs based on attention scores\n",
    "            current_k = min(k, len(subgraph_outputs))\n",
    "            if current_k == 0:\n",
    "                continue\n",
    "\n",
    "            top_k_values, top_k_indices = subgraph_attention_scores.topk(current_k, dim=0, largest=True, sorted=True)\n",
    "            top_k_subgraphs = subgraph_outputs[top_k_indices]\n",
    "\n",
    "            # Aggregate the top-k subgraph outputs (mean aggregation)\n",
    "            final_prediction = top_k_subgraphs.mean(dim=0)\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            final_prediction = torch.softmax(final_prediction, dim=1)\n",
    "\n",
    "            # Apply argmax to find the predicted class\n",
    "            final_prediction_class = final_prediction.argmax(dim=1).item()  # Convert to scalar\n",
    "            true_label = graph.y.item()\n",
    "\n",
    "            all_predictions.append(final_prediction_class)\n",
    "            all_true_labels.append(true_label)\n",
    "\n",
    "            if final_prediction_class == true_label:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    print(correct)\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    \n",
    "    cm = confusion_matrix(all_true_labels, all_predictions)\n",
    "    report = classification_report(all_true_labels, all_predictions, target_names=class_names)\n",
    "\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(report)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class_names = [f'Class {i}' for i in range(dataset.num_classes)]\n",
    "evaluate_model_with_attention(model, test_graphs)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
