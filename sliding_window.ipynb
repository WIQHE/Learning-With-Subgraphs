{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSRC_21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "import torch_geometric.datasets as datasets\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "\n",
    "from torch_geometric.data import Data, DataLoader, InMemoryDataset\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = 'dataset'\n",
    "dataset = torch_geometric.datasets.TUDataset(root=DATASET_PATH, name=\"MSRC_21\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of graphs\n",
    "num_graphs = len(dataset)\n",
    "\n",
    "# Number of classes (unique labels)\n",
    "num_classes = dataset.num_classes\n",
    "\n",
    "# Get the labels (for each graph in the dataset)\n",
    "labels = [data.y.item() for data in dataset]\n",
    "\n",
    "# Calculate the average number of nodes and edges\n",
    "total_nodes = sum(data.num_nodes for data in dataset)\n",
    "total_edges = sum(data.num_edges for data in dataset)\n",
    "avg_nodes = total_nodes / num_graphs\n",
    "avg_edges = total_edges / num_graphs\n",
    "\n",
    "# Display the results\n",
    "print(f\"Number of graphs: {num_graphs}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Labels: {set(labels)}\")\n",
    "print(f\"Average number of nodes: {avg_nodes}\")\n",
    "print(f\"Average number of edges: {avg_edges}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset and split into train and test sets\n",
    "train_idx, test_idx = train_test_split(range(len(dataset)), test_size=0.2, random_state=42)\n",
    "train_dataset = dataset[train_idx]\n",
    "test_dataset = dataset[test_idx]\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, heads=8, dropout=0.6):\n",
    "        super(GAT, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "        \n",
    "        self.conv1 = GATConv(dataset.num_node_features, hidden_channels, heads=heads, dropout=dropout)\n",
    "        # Combine the heads by averaging\n",
    "        self.conv2 = GATConv(hidden_channels * heads, hidden_channels, heads=1, concat=False, dropout=dropout)\n",
    "        self.lin = torch.nn.Linear(hidden_channels, dataset.num_classes)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        # First GAT layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Second GAT layer\n",
    "        x, attn_weights = self.conv2(x, edge_index, return_attention_weights=True)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Global mean pooling\n",
    "        x = global_mean_pool(x, data.batch)\n",
    "\n",
    "        # Classifier\n",
    "        x = self.lin(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1), attn_weights\n",
    "\n",
    "# Set device (use GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the model and move it to the appropriate device\n",
    "hidden_channels = 64\n",
    "model = GAT(hidden_channels=hidden_channels).to(device)\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Training function\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)  # Move the data to GPU\n",
    "        optimizer.zero_grad()\n",
    "        out, _ = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "# Test function\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)  # Move the data to GPU\n",
    "            out, _ = model(data)\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct += (pred == data.y).sum().item()\n",
    "            predictions.extend(pred.tolist())\n",
    "            labels.extend(data.y.tolist())\n",
    "    return correct / len(loader.dataset), predictions, labels\n",
    "\n",
    "# Training loop\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "loss_list = []\n",
    "num_epochs = 200\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    loss = train()\n",
    "    train_acc, _, _ = test(train_loader)\n",
    "    test_acc, predictions, true_labels = test(test_loader)\n",
    "    \n",
    "    train_acc_list.append(train_acc)\n",
    "    test_acc_list.append(test_acc)\n",
    "    loss_list.append(loss)\n",
    "    \n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_labels, predictions, target_names=[f'Class {i}' for i in range(dataset.num_classes)]))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[f'Class {i}' for i in range(dataset.num_classes)],\n",
    "            yticklabels=[f'Class {i}' for i in range(dataset.num_classes)])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Plot accuracy curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, num_epochs + 1), train_acc_list, label=\"Train Accuracy\")\n",
    "plt.plot(range(1, num_epochs + 1), test_acc_list, label=\"Test Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy over Epochs\")\n",
    "plt.show()\n",
    "\n",
    "# Plot loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, num_epochs + 1), loss_list, label=\"Training Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training Loss over Epochs\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for sliding window\n",
    "window_size = 62 # Size of the sliding window (number of nodes in the subgraph)\n",
    "step_size = 5     # Step size for the sliding window\n",
    "\n",
    "# Load the dataset\n",
    "DATASET_PATH = 'dataset'\n",
    "dataset = TUDataset(root=DATASET_PATH, name=\"MSRC_21\")\n",
    "\n",
    "\n",
    "\n",
    "def create_subgraphs(graph, window_size, step_size):\n",
    "    subgraphs = []\n",
    "    num_nodes = graph.num_nodes\n",
    "\n",
    "    for start in range(0, num_nodes - window_size + 1, step_size):\n",
    "        end = start + window_size\n",
    "\n",
    "        # Create a subgraph with the correct node features\n",
    "        subgraph_x = graph.x[start:end]  # Node features\n",
    "\n",
    "        # Create masks for edges within the subgraph\n",
    "        subgraph_edge_index = graph.edge_index\n",
    "        mask = (subgraph_edge_index[0] >= start) & (subgraph_edge_index[0] < end) & \\\n",
    "               (subgraph_edge_index[1] >= start) & (subgraph_edge_index[1] < end)\n",
    "\n",
    "        # Filter edges using the mask\n",
    "        subgraph_edge_index = subgraph_edge_index[:, mask]\n",
    "\n",
    "        # Adjust the indices of the filtered edges to reflect their position in the subgraph\n",
    "        subgraph_edge_index[0] -= start\n",
    "        subgraph_edge_index[1] -= start\n",
    "\n",
    "        # Ensure that edge_index is not empty\n",
    "        if subgraph_edge_index.size(1) == 0:\n",
    "            # If no edges, create a self-loop for each node to avoid empty edge_index\n",
    "            subgraph_edge_index = torch.stack([torch.arange(end - start), torch.arange(end - start)], dim=0)\n",
    "\n",
    "        # Create the subgraph\n",
    "        subgraph = Data(x=subgraph_x, edge_index=subgraph_edge_index, y=graph.y)  # Keep the label\n",
    "        \n",
    "        # **Add the original node indices as an attribute**\n",
    "        subgraph.original_node_indices = list(range(start, end))\n",
    "\n",
    "        subgraphs.append(subgraph)\n",
    "\n",
    "    return subgraphs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SubgraphDataset(InMemoryDataset):\n",
    "    def __init__(self, dataset):\n",
    "        super(SubgraphDataset, self).__init__(root=DATASET_PATH)\n",
    "        # Flatten list of subgraphs\n",
    "        self.data_list = []\n",
    "        self.labels = []\n",
    "\n",
    "        for graph in dataset:\n",
    "            subgraphs = create_subgraphs(graph, window_size, step_size)\n",
    "            self.data_list.extend(subgraphs)\n",
    "            self.labels.extend([graph.y] * len(subgraphs))  # Add label for each subgraph\n",
    "\n",
    "        # Convert to tensor\n",
    "        self.data, self.slices = self.collate(self.data_list)\n",
    "\n",
    "    def get_labels(self):\n",
    "        return torch.tensor(self.labels)\n",
    "\n",
    "\n",
    "train_graphs, test_graphs = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "train_subgraph_dataset = SubgraphDataset(train_graphs)\n",
    "test_subgraph_dataset = SubgraphDataset(test_graphs)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_subgraph_dataset.data_list, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_subgraph_dataset.data_list, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, optimizer, and loss function\n",
    "hidden_channels = 64\n",
    "heads = 8\n",
    "dropout = 0.6\n",
    "model = GAT(hidden_channels=hidden_channels, heads=heads, dropout=dropout)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Move data to device\n",
    "def move_to_device(batch, device):\n",
    "    batch = batch.to(device)\n",
    "    return batch\n",
    "\n",
    "# Weakly supervised training\n",
    "model.train()\n",
    "for epoch in range(50):  # Number of epochs\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in train_loader:\n",
    "        batch = move_to_device(batch, device)\n",
    "        optimizer.zero_grad()\n",
    "        out, attn_weights = model(batch)\n",
    "        loss = criterion(out, batch.y)  # Compute loss using graph labels\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        \n",
    "        _, predicted = torch.max(out, dim=1)\n",
    "        correct += (predicted == batch.y).sum().item()\n",
    "        total += batch.y.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = correct / total\n",
    "    print(f'Epoch {epoch + 1}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Evaluation function to select top-k subgraphs based on attention weights\n",
    "def evaluate_model_with_attention(model, dataset, k=3):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for graph in dataset:\n",
    "            subgraphs = create_subgraphs(graph, window_size, step_size)\n",
    "            subgraph_outputs = []\n",
    "            subgraph_attention_scores = []\n",
    "\n",
    "            for subgraph in subgraphs:\n",
    "                subgraph = subgraph.to(device)\n",
    "                output, attn_weights = model(subgraph)\n",
    "                \n",
    "                # Extract attention weights\n",
    "                if isinstance(attn_weights, tuple) or isinstance(attn_weights, list):\n",
    "                    attention_tensor = attn_weights[-1]\n",
    "                else:\n",
    "                    attention_tensor = attn_weights\n",
    "\n",
    "                # Compute a single attention score for the subgraph\n",
    "                attention_score = attention_tensor.mean().item()\n",
    "                \n",
    "                subgraph_outputs.append(output.unsqueeze(0))\n",
    "                subgraph_attention_scores.append(attention_score)\n",
    "\n",
    "            if not subgraph_outputs:\n",
    "                continue\n",
    "\n",
    "            subgraph_outputs = torch.cat(subgraph_outputs, dim=0)\n",
    "            subgraph_attention_scores = torch.tensor(subgraph_attention_scores)\n",
    "\n",
    "            # Select top-k subgraphs based on attention scores\n",
    "            current_k = min(k, len(subgraph_outputs))\n",
    "            if current_k == 0:\n",
    "                continue\n",
    "\n",
    "            top_k_values, top_k_indices = subgraph_attention_scores.topk(current_k, dim=0, largest=True, sorted=True)\n",
    "            top_k_subgraphs = subgraph_outputs[top_k_indices]\n",
    "\n",
    "            # Aggregate the top-k subgraph outputs (mean aggregation)\n",
    "            final_prediction = top_k_subgraphs.mean(dim=0)\n",
    "\n",
    "            \n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            final_prediction = torch.softmax(final_prediction, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "            # Apply argmax to find the predicted class\n",
    "            final_prediction_class = final_prediction.argmax(dim=1).item()  # Convert to scalar\n",
    "\n",
    "            \n",
    "            true_label = graph.y.item()\n",
    "\n",
    "            all_predictions.append(final_prediction_class)\n",
    "            all_true_labels.append(true_label)\n",
    "\n",
    "            if final_prediction_class == true_label:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    cm = confusion_matrix(all_true_labels, all_predictions)\n",
    "    report = classification_report(all_true_labels, all_predictions,target_names=class_names)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(20)\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(report)\n",
    "\n",
    "# Evaluate the model using the top-k subgraph selection\n",
    "class_names = [f'Class {i}' for i in range(dataset.num_classes)]\n",
    "evaluate_model_with_attention(model, test_graphs,k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "def visualize_graph(nx_graph, title, pos, color='lightblue', with_labels=False, node_size=300, edge_color='gray'):\n",
    "    \"\"\"\n",
    "    Function to visualize the entire graph.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    nx.draw(nx_graph, pos, node_color=color, with_labels=with_labels, node_size=node_size, edge_color=edge_color)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def visualize_subgraph_highlight(nx_graph, subgraph, pos, color, title):\n",
    "    \"\"\"\n",
    "    Function to visualize a subgraph with a specific color.\n",
    "    Highlights the nodes and edges corresponding to the subgraph.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Draw the full graph with light gray nodes and edges\n",
    "    nx.draw(nx_graph, pos, node_color='lightgray', with_labels=False, node_size=300, edge_color='lightgray')  \n",
    "    \n",
    "    # Highlight the subgraph nodes and edges\n",
    "    subgraph_nodes = subgraph.original_node_indices\n",
    "    subgraph_edges = subgraph.edge_index.cpu().numpy().T  # Convert to numpy array\n",
    "    # Map subgraph edge indices back to original graph node indices\n",
    "    original_edges = [(subgraph.original_node_indices[src], subgraph.original_node_indices[dst]) for src, dst in subgraph_edges]\n",
    "    \n",
    "    nx.draw_networkx_nodes(nx_graph, pos, nodelist=subgraph_nodes, node_color=color, node_size=300)\n",
    "    nx.draw_networkx_edges(nx_graph, pos, edgelist=original_edges, edge_color=color, width=2)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_and_visualize_top_k_separately(model, dataset, k=3, random_seed=42):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for graph in dataset:\n",
    "            subgraphs = create_subgraphs(graph, window_size, step_size)\n",
    "            print(f'Number of subgraphs: {len(subgraphs)}')\n",
    "            subgraph_outputs = []\n",
    "            subgraph_attention_scores = []\n",
    "\n",
    "            # Convert the original PyTorch Geometric graph to a NetworkX graph\n",
    "            nx_graph = to_networkx(graph, to_undirected=True)\n",
    "            pos = nx.spring_layout(nx_graph, seed=random_seed)  # Generate positions once for consistency\n",
    "\n",
    "            for subgraph in subgraphs:\n",
    "                subgraph = subgraph.to(device)\n",
    "                output, attn_weights = model(subgraph)\n",
    "\n",
    "                # Extract attention weights and compute a single attention score\n",
    "                if isinstance(attn_weights, (tuple, list)):\n",
    "                    attention_tensor = attn_weights[-1]  # Use the last attention layer's weights\n",
    "                else:\n",
    "                    attention_tensor = attn_weights\n",
    "\n",
    "                attention_score = attention_tensor.mean().item()\n",
    "                subgraph_outputs.append(output.unsqueeze(0))\n",
    "                subgraph_attention_scores.append(attention_score)\n",
    "\n",
    "            if not subgraph_outputs:\n",
    "                continue\n",
    "\n",
    "            subgraph_outputs = torch.cat(subgraph_outputs, dim=0)\n",
    "            subgraph_attention_scores = torch.tensor(subgraph_attention_scores)\n",
    "\n",
    "            # Select top-k subgraphs based on attention scores\n",
    "            current_k = min(k, len(subgraph_outputs))\n",
    "            if current_k == 0:\n",
    "                continue\n",
    "\n",
    "            top_k_values, top_k_indices = subgraph_attention_scores.topk(current_k, dim=0, largest=True, sorted=True)\n",
    "            \n",
    "            # original graph\n",
    "            visualize_graph(nx_graph, title=\"Original Graph\", pos=pos)\n",
    "\n",
    "            # top-k subgraphs\n",
    "            colors = ['red', 'green', 'blue']  # Extend this list if k > 3\n",
    "            for i, idx in enumerate(top_k_indices):\n",
    "                top_subgraph = subgraphs[idx]\n",
    "                visualize_subgraph_highlight(nx_graph, top_subgraph, pos, color=colors[i % len(colors)], title=f'Top-{i+1} Subgraph (Attention Score: {subgraph_attention_scores[idx]:.4f})')\n",
    "\n",
    "            \n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "evaluate_and_visualize_top_k_separately(model, test_graphs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
